# Style Transfer Model (Classic Korean & Classic Paintings)

This repository implements a style transfer model using CycleGAN architecture to convert images between Korean traditional style paintings and Monet-style paintings.

## Overview

This project is designed to perform **image-to-image translation** between two different artistic domains:

- **Domain A:** Monet-style paintings
- **Domain B:** Korean traditional paintings

The model is trained using the **CycleGAN architecture** with additional style and content extraction using a pre-trained **VGG19 model**.

---

## Features

- Image translation between Monet-style and Korean traditional painting styles.
- Image-to-image translation using two generators and two discriminators.
- Style and content extraction using a pre-trained VGG19 network.
- Implementation of GAN, Cycle Consistency, and Identity loss functions.

---

## Model Architecture

### Generators

- **ResNet-based architecture**
- 9 residual blocks with instance normalization and ReLU activation functions.
- Downsampling and upsampling layers for image transformation.

### Discriminators

- **PatchGAN architecture**
- Multiple convolutional layers with LeakyReLU activation.
- Determines if an image is real or generated by a generator.

### VGG19 Style Extractor

- Uses a pre-trained VGG19 network to extract style and content features.
- Style features are captured using **Gram matrices**.

---

## Loss Functions

- **GAN Loss:** Measures how well the generator fools the discriminator.
- **Cycle Consistency Loss:** Ensures that translating an image to another domain and back results in the original image.
- **Identity Loss:** Encourages the generator to preserve the identity of the input image if it is already in the target domain.

---

## Installation

```bash
# Clone the repository
$ git clone https://github.com/your-username/your-repository-name.git

# Navigate to the project directory
$ cd your-repository-name

# Install dependencies
$ pip install -r requirements.txt
```

---

## Training

1. **Prepare your datasets**: Store your training images in two folders, one for each domain (e.g., `data_monet` and `data_korean_resized`).
2. **Run the training script**:

```bash
$ python train.py
```

### Hyperparameters

- Learning Rate: 0.0002
- Batch Size: 1
- Number of Epochs: 300

---

## Image Generation

```python
from PIL import Image
from torchvision import transforms
import torch

# Load image
img = Image.open('path/to/your/image.jpg').convert('RGB')

# Preprocess
transform = transforms.Compose([
    transforms.Resize(256),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
img_tensor = transform(img).unsqueeze(0).to(device)

# Generate image
with torch.no_grad():
    output_tensor = G_A2B(img_tensor)

# Post-process and save
output_tensor = (output_tensor.squeeze().cpu() + 1) / 2
output_img = transforms.ToPILImage()(output_tensor)
output_img.save('result.jpg')
```

---

## Results

Include example images of your trained model's performance here.

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- This implementation is inspired by the original CycleGAN paper: [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593).
- Pre-trained VGG19 model from PyTorch.

---

